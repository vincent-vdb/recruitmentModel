{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "rawfile = pd.read_csv(\"bizCase/Business_case_data.csv\")\n",
    "file = rawfile.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataset(file):\n",
    "    # First remove bad values of head translation\n",
    "    file = file[file.pose_Tx > -1000]\n",
    "    # Remove infinite values of hand distance\n",
    "    file = file[file.DistanceHands_1_adjusted != float(\"inf\")]\n",
    "    # Replace the NaN values with zeros (sic)\n",
    "    file['DistanceHands_1_adjusted'] = file['DistanceHands_1_adjusted'].fillna(0)\n",
    "    file['DistanceHands_2_adjusted'] = file['DistanceHands_2_adjusted'].fillna(0)\n",
    "\n",
    "    # Trying to add an empirical threshold on confidence... would need further investigation\n",
    "    file = file[file.confidence>0.8]\n",
    "    # Replace the IDs with easier ones ranging from 1 to 219\n",
    "    file['easyID'] = file.candidate_id.astype('category').cat.rename_categories(range(1, file.candidate_id.nunique()+1))\n",
    "    # Replace question IDs with easier values for understanding\n",
    "    file['easyQID'] = file.question_id.astype('category').cat.rename_categories(range(1, file.question_id.nunique()+1))\n",
    "\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gaze, head translation and head rotation distance\n",
    "def computeDistance(file):\n",
    "    file['gazeDist'] = np.sqrt(np.square(file['gaze_angle_x']-np.mean(file['gaze_angle_x']))+np.square(file['gaze_angle_y']-np.mean(file['gaze_angle_y'])))\n",
    "    file['Tdist'] = np.sqrt(np.square(file['pose_Tx'])+np.square(file['pose_Ty'])+np.square(file['pose_Tz']-np.mean(file['pose_Tz'])))\n",
    "    file['Rdist'] = np.sqrt(np.square(file['pose_Rx'])+np.square(file['pose_Ry'])+np.square(file['pose_Rz']))\n",
    "    \n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the features\n",
    "def rescaleFeatures(file):\n",
    "\n",
    "    rescaleFeatures = ['gazeDist', 'Tdist', 'Rdist', 'AU01_r','AU02_r','AU04_r','AU05_r','AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r','AU15_r','AU17_r','AU20_r','AU23_r','AU25_r','AU26_r','AU45_r']\n",
    "    file[rescaleFeatures] /= file[rescaleFeatures].max()\n",
    "    \n",
    "    return file\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeScore(clf, X, y):\n",
    "  xval = cross_val_score(clf, X, y, cv=2)\n",
    "  return xval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getCandidateMeanDataset(file):\n",
    "\n",
    "    # Number of candidates\n",
    "    nbCand = file['easyID'].nunique()\n",
    "\n",
    "    # selected features for all frames\n",
    "    selectedFeatures = ['gazeDist', 'Tdist', 'Rdist', 'AU01_r','AU02_r','AU04_r','AU05_r','AU06_r','AU07_r','AU09_r','AU10_r','AU12_r','AU14_r','AU15_r','AU17_r','AU20_r','AU23_r','AU25_r','AU26_r','AU45_r','AU01_c','AU02_c','AU04_c','AU05_c','AU06_c','AU07_c','AU09_c','AU10_c','AU12_c','AU14_c','AU15_c','AU17_c','AU20_c','AU23_c','AU25_c','AU26_c','AU28_c','AU45_c','Proba_hands_1_binned', 'Proba_hands_2_binned', 'confidence', 'DistanceHands_1_adjusted', 'DistanceHands_2_adjusted']\n",
    "    # kept features per candidate (not for all frames)\n",
    "    finalFeatures = ['gazeDistm', 'Tdistm', 'Rdistm', 'AU01_rm','AU02_rm','AU04_rm','AU05_rm','AU06_rm','AU07_rm','AU09_rm','AU10_rm','AU12_rm','AU14_rm','AU15_rm','AU17_rm','AU20_rm','AU23_rm','AU25_rm','AU26_rm','AU45_rm','AU01_cm','AU02_cm','AU04_cm','AU05_cm','AU06_cm','AU07_cm','AU09_cm','AU10_cm','AU12_cm','AU14_cm','AU15_cm','AU17_cm','AU20_cm','AU23_cm','AU25_cm','AU26_cm','AU28_cm','AU45_cm','Proba_hands_1_binnedm', 'Proba_hands_2_binnedm', 'confidencem', 'DistanceHands_1_adjustedm', 'DistanceHands_2_adjustedm','gazeDists', 'Tdists', 'Rdists', 'AU01_rs','AU02_rs','AU04_rs','AU05_rs','AU06_rs','AU07_rs','AU09_rs','AU10_rs','AU12_rs','AU14_rs','AU15_rs','AU17_rs','AU20_rs','AU23_rs','AU25_rs','AU26_rs','AU45_rs','AU01_cs','AU02_cs','AU04_cs','AU05_cs','AU06_cs','AU07_cs','AU09_cs','AU10_cs','AU12_cs','AU14_cs','AU15_cs','AU17_cs','AU20_cs','AU23_cs','AU25_cs','AU26_cs','AU28_cs','AU45_cs','Proba_hands_1_binneds', 'Proba_hands_2_binneds', 'confidences', 'DistanceHands_1_adjusteds', 'DistanceHands_2_adjusteds']\n",
    "\n",
    "    # data per candidate\n",
    "    xtrain = pd.DataFrame(index=range(0,nbCand), columns=finalFeatures)\n",
    "    ytrain = pd.DataFrame(index=range(0,nbCand), columns=['label'])\n",
    "\n",
    "    # Loop over all candidates\n",
    "    for i in range(1, nbCand+1):\n",
    "        for feat in selectedFeatures:\n",
    "            xtrain[feat+'m'].iloc[i-1] = file[feat][file.easyID == i].mean()\n",
    "            xtrain[feat+'s'].iloc[i-1] = file[feat][file.easyID == i].std()\n",
    "\n",
    "        ytrain['label'].iloc[i-1] = file['label'][file.easyID == i].mean()\n",
    "    \n",
    "    return xtrain, ytrain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the whole model and output some precision values\n",
    "def computeModel(file):\n",
    "    file = cleanDataset(file)\n",
    "    file = computeDistance(file)\n",
    "    file = rescaleFeatures(file)\n",
    "    xtrain, ytrain = getCandidateMeanDataset(file)\n",
    "\n",
    "    # Output precision for logistic regression, random forest, gradient boosting\n",
    "    # SVM and k-NN\n",
    "    lr = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=.1, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=1000, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "    print(\"logistic regression: \",np.mean(computeScore(lr, xtrain, np.ravel(ytrain))))\n",
    "    rdf = RandomForestClassifier(n_estimators=1000, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "    print(\"random forest classification: \",np.mean(computeScore(rdf, xtrain, np.ravel(ytrain))))\n",
    "    grad = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=10000, subsample=0.9, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto')\n",
    "    print(\"gradient boosting: \",np.mean(computeScore(grad, xtrain, np.ravel(ytrain)))) \n",
    "    mySVC = SVC(C=.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "    print(\"SVM Classifier: \", np.mean(computeScore(mySVC, xtrain, np.ravel(ytrain))))\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=1)\n",
    "    print(\"k-NN Classifier: \", np.mean(computeScore(knn, xtrain, np.ravel(ytrain))))\n",
    "\n",
    "    return xtrain, ytrain   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/pandas/core/computation/check.py:17: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('logistic regression: ', 0.6826923076923077)\n",
      "('random forest classification: ', 0.6971153846153846)\n",
      "('gradient boosting: ', 0.5961538461538461)\n",
      "('SVM Classifier: ', 0.6826923076923077)\n",
      "('k-NN Classifier: ', 0.6009615384615384)\n"
     ]
    }
   ],
   "source": [
    "# finally execute the computation\n",
    "xtrain, ytrain = computeModel(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
